{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaUFZbZYq6-e",
        "outputId": "e26754c9-5be7-450c-9ef8-41573c91e00a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing missing packages: ['rdkit', 'scikit-learn', 'PyTDC']\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# End-to-end ML pipeline for:\n",
        "# A) Target inhibition (ChEMBL),\n",
        "# B) Water solubility (TDC Solubility_AqSolDB),\n",
        "# C) Human half-life (TDC Half_Life_Human),\n",
        "# D) hERG inhibition (TDC hERG)\n",
        "# =========================\n",
        "from typing import List\n",
        "import sys, subprocess\n",
        "\n",
        "# ---------- Dependency management ----------\n",
        "def ensure_installed(pkgs: List[str]):\n",
        "    import importlib\n",
        "    to_install = []\n",
        "    for p in pkgs:\n",
        "        mod_name = p.split(\"==\")[0]\n",
        "        try:\n",
        "            importlib.import_module(mod_name.replace(\"-\", \"_\"))\n",
        "        except ImportError:\n",
        "            to_install.append(p)\n",
        "    if to_install:\n",
        "        print(\"Installing missing packages:\", to_install)\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + to_install)\n",
        "\n",
        "ensure_installed([\n",
        "    \"rdkit\",       # CHANGES_MADE_01: rdkit-pypi -> rdkit\n",
        "    \"xgboost\",\n",
        "    \"scikit-learn\",\n",
        "    \"pandas\",\n",
        "    \"numpy\",\n",
        "    \"matplotlib\",\n",
        "    \"seaborn\",\n",
        "    \"tqdm\",\n",
        "    \"requests\",\n",
        "    \"PyTDC\"\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/UAB/StanfordAI/data .\n",
        "# !cp -r /content/drive/MyDrive/UAB/StanfordAI/figures .\n",
        "# !cp -r /content/drive/MyDrive/UAB/StanfordAI/models .\n",
        "# !cp -r /content/drive/MyDrive/UAB/StanfordAI/results ."
      ],
      "metadata": {
        "id": "Et-HK4ZUJInz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hPe5CryUrbMZ"
      },
      "outputs": [],
      "source": [
        "# ---------- Imports ----------\n",
        "import os, sys, time, json, math, random, subprocess, warnings, pickle, joblib\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from rdkit import RDLogger\n",
        "RDLogger.DisableLog('rdApp.*')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score, roc_curve, precision_recall_curve,\n",
        "    confusion_matrix, f1_score, matthews_corrcoef, balanced_accuracy_score,\n",
        "    brier_score_loss, r2_score, mean_absolute_error\n",
        ")\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.utils import resample\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors, rdMolDescriptors, AllChem\n",
        "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
        "from rdkit.Chem.MolStandardize import rdMolStandardize          # CHANGES_MADE_02 from rdkit.Chem import rdMolStandardize\n",
        "\n",
        "# TDC\n",
        "from tdc.single_pred import ADME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iX7oDu8xuJbk"
      },
      "outputs": [],
      "source": [
        "# ---------- Global config ----------\n",
        "RANDOM_SEEDS = [13, 17, 23, 29, 31]  # 5 repeats\n",
        "MODEL_DIR = \"models\"\n",
        "RESULTS_DIR = \"results\"\n",
        "FIG_DIR = \"figures\"\n",
        "DATA_DIR = \"data\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "os.makedirs(FIG_DIR, exist_ok=True)\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "TARGET = \"any_target\"\n",
        "\n",
        "# ---------- Chemistry utilities ----------\n",
        "def standardize_smiles(smiles: str) -> Optional[str]:\n",
        "    if smiles is None or not isinstance(smiles, str) or len(smiles.strip()) == 0:\n",
        "        return None\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            return None\n",
        "        # Cleanup & largest fragment\n",
        "        clean_mol = rdMolStandardize.Cleanup(mol)\n",
        "        lfc = rdMolStandardize.LargestFragmentChooser()\n",
        "        mol = lfc.choose(clean_mol)\n",
        "        # Neutralize\n",
        "        uncharger = rdMolStandardize.Uncharger()\n",
        "        mol = uncharger.uncharge(mol)\n",
        "        # Canonical SMILES\n",
        "        can = Chem.MolToSmiles(mol, isomericSmiles=True, canonical=True)\n",
        "        return can\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def compute_descriptors(smiles_list: List[str]) -> Tuple[pd.DataFrame, List[str]]:\n",
        "    # Compute RDKit 2D descriptors\n",
        "    desc_names = [d[0] for d in Descriptors.descList]\n",
        "    records = []\n",
        "    valid_idx = []\n",
        "    for i, smi in enumerate(tqdm(smiles_list, desc=\"Computing RDKit 2D descriptors\")):\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if mol is None:\n",
        "            records.append([np.nan]*len(desc_names))\n",
        "            continue\n",
        "        vals = []\n",
        "        for (_, func) in Descriptors.descList:\n",
        "            try:\n",
        "                vals.append(func(mol))\n",
        "            except Exception:\n",
        "                vals.append(np.nan)\n",
        "        records.append(vals)\n",
        "        valid_idx.append(i)\n",
        "    df = pd.DataFrame(records, columns=desc_names)\n",
        "    # Remove columns that are all NaN or zero variance\n",
        "    df = df.loc[:, ~df.isna().all()]\n",
        "    nunique = df.nunique(dropna=True)\n",
        "    df = df.loc[:, nunique > 1]\n",
        "    # Replace inf with nan\n",
        "    df = df.replace([np.inf, -np.inf], np.nan)\n",
        "    # Impute with column median\n",
        "    df = df.fillna(df.median(numeric_only=True))\n",
        "    return df, list(df.columns)\n",
        "\n",
        "def morgan_fp(smiles_list: List[str], radius=2, nBits=2048) -> np.ndarray:\n",
        "    fps = np.zeros((len(smiles_list), nBits), dtype=np.uint8)\n",
        "    for i, smi in enumerate(tqdm(smiles_list, desc=f\"Computing MorganFP r{radius} n{nBits}\")):\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if mol is None:\n",
        "            continue\n",
        "        bv = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits)\n",
        "        arr = np.zeros((1, nBits), dtype=np.uint8)\n",
        "        Chem.DataStructs.ConvertToNumpyArray(bv, arr[0])\n",
        "        fps[i, :] = arr\n",
        "    return fps\n",
        "\n",
        "def murcko_scaffold(smiles: str) -> str:\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            return \"\"\n",
        "        scaf = MurckoScaffold.MurckoScaffoldSmiles(mol=mol, includeChirality=False)\n",
        "        if scaf is None:\n",
        "            return \"\"\n",
        "        return scaf\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def scaffold_split_indices(smiles: List[str], train_frac=0.72, val_frac=0.08, test_frac=0.20, seed=42):\n",
        "    assert abs(train_frac + val_frac + test_frac - 1.0) < 1e-6\n",
        "    rng = np.random.RandomState(seed)\n",
        "    scaffolds = {}\n",
        "    for idx, smi in enumerate(smiles):\n",
        "        scaf = murcko_scaffold(smi)\n",
        "        scaffolds.setdefault(scaf, []).append(idx)\n",
        "    # Shuffle scaffolds deterministically\n",
        "    scaf_items = list(scaffolds.items())\n",
        "    rng.shuffle(scaf_items)\n",
        "    # Sort by scaffold size (large first) to fill splits\n",
        "    scaf_items.sort(key=lambda x: len(x[1]), reverse=True)\n",
        "\n",
        "    n = len(smiles)\n",
        "    train_idx, val_idx, test_idx = [], [], []\n",
        "    train_target = int(train_frac * n)\n",
        "    val_target = int(val_frac * n)\n",
        "    # Greedy fill\n",
        "    for scaf, idxs in scaf_items:\n",
        "        if len(train_idx) + len(idxs) <= train_target:\n",
        "            train_idx.extend(idxs)\n",
        "        elif len(val_idx) + len(idxs) <= val_target:\n",
        "            val_idx.extend(idxs)\n",
        "        else:\n",
        "            test_idx.extend(idxs)\n",
        "    # In case of rounding leftovers, move extras to test\n",
        "    leftover = set(range(n)) - set(train_idx) - set(val_idx) - set(test_idx)\n",
        "    test_idx.extend(list(leftover))\n",
        "    return np.array(train_idx), np.array(val_idx), np.array(test_idx)\n",
        "\n",
        "# ---------- Feature builder ----------\n",
        "def build_features(smiles: List[str], radius=2, nBits=2048):\n",
        "    df_desc, desc_cols = compute_descriptors(smiles)\n",
        "    scaler = RobustScaler()\n",
        "    X_desc = scaler.fit_transform(df_desc.values.astype(float))\n",
        "    X_fp = morgan_fp(smiles, radius=radius, nBits=nBits).astype(np.float32)\n",
        "    X = np.hstack([X_desc, X_fp])\n",
        "    feature_meta = {\n",
        "        \"desc_cols\": desc_cols,\n",
        "        \"desc_scaler_center_\": scaler.center_.tolist(),\n",
        "        \"desc_scaler_scale_\": scaler.scale_.tolist(),\n",
        "        \"fp_radius\": radius,\n",
        "        \"fp_nBits\": nBits,\n",
        "        \"n_desc\": len(desc_cols),\n",
        "        \"n_fp\": nBits,\n",
        "        \"n_total\": X.shape[1]\n",
        "    }\n",
        "    return X, df_desc, scaler, feature_meta\n",
        "\n",
        "# ---------- Evaluation ----------\n",
        "def classification_metrics(y_true, y_proba, threshold=0.5):\n",
        "    y_pred = (y_proba >= threshold).astype(int)\n",
        "    metrics = {}\n",
        "    try:\n",
        "        metrics[\"auroc\"] = roc_auc_score(y_true, y_proba)\n",
        "    except Exception:\n",
        "        metrics[\"auroc\"] = np.nan\n",
        "    try:\n",
        "        metrics[\"auprc\"] = average_precision_score(y_true, y_proba)\n",
        "    except Exception:\n",
        "        metrics[\"auprc\"] = np.nan\n",
        "    metrics[\"f1\"] = f1_score(y_true, y_pred, zero_division=0)\n",
        "    metrics[\"mcc\"] = matthews_corrcoef(y_true, y_pred)\n",
        "    metrics[\"balanced_acc\"] = balanced_accuracy_score(y_true, y_pred)\n",
        "    try:\n",
        "        metrics[\"brier\"] = brier_score_loss(y_true, y_proba)\n",
        "    except Exception:\n",
        "        metrics[\"brier\"] = np.nan\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
        "    metrics.update({\"tn\": tn, \"fp\": fp, \"fn\": fn, \"tp\": tp, \"threshold\": threshold})\n",
        "    return metrics\n",
        "\n",
        "def regression_metrics(y_true, y_pred):\n",
        "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    try:\n",
        "        rho, _ = spearmanr(y_true, y_pred)\n",
        "    except Exception:\n",
        "        rho = np.nan\n",
        "    return {\"rmse\": rmse, \"mae\": mae, \"r2\": r2, \"spearman\": rho}\n",
        "\n",
        "# ---------- Plotting ----------\n",
        "def set_pub_style():\n",
        "    plt.style.use(\"seaborn-v0_8-whitegrid\")           # CHANGES_MADE_04: seaborn-whitegrid\n",
        "    sns.set_context(\"talk\")\n",
        "    plt.rcParams[\"figure.dpi\"] = 150\n",
        "    plt.rcParams[\"savefig.dpi\"] = 300\n",
        "    plt.rcParams[\"font.size\"] = 12\n",
        "    plt.rcParams[\"axes.labelsize\"] = 12\n",
        "    plt.rcParams[\"legend.fontsize\"] = 10\n",
        "    plt.rcParams[\"axes.titlesize\"] = 14\n",
        "\n",
        "def plot_roc_pr_curves(y_true_list, y_proba_list, title_prefix, out_prefix):\n",
        "    set_pub_style()\n",
        "    # ROC\n",
        "    plt.figure(figsize=(6,5))\n",
        "    for i, (yt, yp) in enumerate(zip(y_true_list, y_proba_list)):\n",
        "        fpr, tpr, _ = roc_curve(yt, yp)\n",
        "        auc = roc_auc_score(yt, yp)\n",
        "        plt.plot(fpr, tpr, lw=1.5, label=f\"Split {i+1} (AUROC={auc:.3f})\")\n",
        "    plt.plot([0,1], [0,1], 'k--', lw=1)\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(f\"{title_prefix}: ROC Curves\")\n",
        "    plt.legend(frameon=True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{FIG_DIR}/{out_prefix}_roc.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # PR\n",
        "    plt.figure(figsize=(6,5))\n",
        "    for i, (yt, yp) in enumerate(zip(y_true_list, y_proba_list)):\n",
        "        prec, rec, _ = precision_recall_curve(yt, yp)\n",
        "        aupr = average_precision_score(yt, yp)\n",
        "        plt.plot(rec, prec, lw=1.5, label=f\"Split {i+1} (AUPRC={aupr:.3f})\")\n",
        "    base = np.mean(np.concatenate(y_true_list))\n",
        "    plt.hlines(base, 0, 1, colors='k', linestyles='--', label=f\"Baseline={base:.3f}\")\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.title(f\"{title_prefix}: Precision-Recall Curves\")\n",
        "    plt.legend(frameon=True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{FIG_DIR}/{out_prefix}_pr.png\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_conf_matrix(y_true, y_pred, title, out_path):\n",
        "    set_pub_style()\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
        "    plt.figure(figsize=(5,4))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
        "                xticklabels=[\"Pred 0\",\"Pred 1\"], yticklabels=[\"True 0\",\"True 1\"])\n",
        "    plt.title(title)\n",
        "    plt.ylabel(\"True label\")\n",
        "    plt.xlabel(\"Predicted label\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path)\n",
        "    plt.close()\n",
        "\n",
        "def plot_parity(y_true_list, y_pred_list, title, out_path, xlabel=\"True\", ylabel=\"Predicted\"):\n",
        "    set_pub_style()\n",
        "    plt.figure(figsize=(5.5,5))\n",
        "    for i, (yt, yp) in enumerate(zip(y_true_list, y_pred_list)):\n",
        "        plt.scatter(yt, yp, s=12, alpha=0.5, label=f\"Split {i+1}\")\n",
        "    mn = min(np.min(np.concatenate(y_true_list)), np.min(np.concatenate(y_pred_list)))\n",
        "    mx = max(np.max(np.concatenate(y_true_list)), np.max(np.concatenate(y_pred_list)))\n",
        "    plt.plot([mn, mx], [mn, mx], 'k--', lw=1)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.legend(frameon=True, markerscale=1, fontsize=9)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path)\n",
        "    plt.close()\n",
        "\n",
        "def plot_residuals(y_true_list, y_pred_list, title, out_path):\n",
        "    set_pub_style()\n",
        "    plt.figure(figsize=(6,4))\n",
        "    res_all = []\n",
        "    for yt, yp in zip(y_true_list, y_pred_list):\n",
        "        res_all.append(yp - yt)\n",
        "    res = np.concatenate(res_all)\n",
        "    sns.histplot(res, bins=40, kde=True, color=\"slateblue\")\n",
        "    plt.axvline(0, color=\"k\", linestyle=\"--\", lw=1)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Residual (Pred - True)\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path)\n",
        "    plt.close()\n",
        "\n",
        "def plot_feature_importance(model, X_cols, X, y, title, out_path, n_top=20, random_state=42):\n",
        "    set_pub_style()\n",
        "    # Permutation importance on descriptor block only (interpretable)\n",
        "    r = permutation_importance(model, X, y, n_repeats=10, random_state=random_state, scoring=None)\n",
        "    importances = pd.Series(r.importances_mean, index=X_cols)\n",
        "    imp = importances.sort_values(ascending=False).head(n_top)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.barplot(x=imp.values, y=imp.index, color=\"teal\")\n",
        "    plt.xlabel(\"Permutation importance (mean decrease in score)\")\n",
        "    plt.ylabel(\"Feature\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path)\n",
        "    plt.close()\n",
        "\n",
        "# ---------- Dataset loaders ----------\n",
        "def load_tdc_solubility():\n",
        "    print(\"Loading TDC Solubility_AqSolDB...\")\n",
        "    data = ADME(name=\"Solubility_AqSolDB\").get_data()\n",
        "    # columns: Drug (SMILES), Y (logS)\n",
        "    df = pd.DataFrame(data)\n",
        "    df = df.rename(columns={\"Drug\": \"smiles\", \"Y\": \"y\"})\n",
        "    return df\n",
        "\n",
        "def load_tdc_half_life():\n",
        "    print(\"Loading TDC Half_Life_Obach...\")\n",
        "    data = ADME(name=\"Half_Life_Obach\").get_data()      # CHANGES_MADE_06 Half_Life_Obach from Half_Life_Human\n",
        "    df = pd.DataFrame(data)\n",
        "    df = df.rename(columns={\"Drug\": \"smiles\", \"Y\": \"y\"})\n",
        "    return df\n",
        "\n",
        "def load_tdc_herg():\n",
        "    print(\"Loading TDC hERG...\")\n",
        "\n",
        "    from tdc.single_pred import Tox\n",
        "    data = Tox(name = 'hERG_Karim').get_data()\n",
        "\n",
        "    # data = ADME(name=\"hERG_Karim\").get_data()                 # CHANGES_MADE_08: hERG_Karim from hERG; Tox to ADME\n",
        "    df = pd.DataFrame(data)\n",
        "    df = df.rename(columns={\"Drug\": \"smiles\", \"Y\": \"y\"})\n",
        "    return df\n",
        "\n",
        "# ---------- ChEMBL ----------\n",
        "CHEMBL_API = \"https://www.ebi.ac.uk/chembl/api/data\"\n",
        "\n",
        "def chembl_get_target_id(query_target):\n",
        "    query_target = query_target.replace(\" \", \"%20\")\n",
        "    url = f\"{CHEMBL_API}/target/search.json?q={query_target}\"\n",
        "    r = requests.get(url, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    js = r.json()\n",
        "    for rec in js.get(\"targets\", []):\n",
        "        if rec.get(\"target_type\") == \"SINGLE PROTEIN\" and rec.get(\"organism\") == \"Homo sapiens\":\n",
        "            return rec.get(\"target_chembl_id\")\n",
        "    # fallback: just choose the first\n",
        "    if js.get(\"targets\"):\n",
        "        return js[\"targets\"][0][\"target_chembl_id\"]\n",
        "    raise RuntimeError(f\"Target {query_target} not found in ChEMBL search.\")\n",
        "\n",
        "def chembl_fetch_activities_for_target(target_id, page_size=1000, max_pages=50):\n",
        "    # get activities with pchembl_value present if possible\n",
        "    activities = []\n",
        "    for page in range(max_pages):\n",
        "        offset = page * page_size\n",
        "        url = f\"{CHEMBL_API}/activity.json?target_chembl_id={target_id}&limit={page_size}&offset={offset}\"\n",
        "        r = requests.get(url, timeout=60)\n",
        "        r.raise_for_status()\n",
        "        js = r.json()\n",
        "        items = js.get(\"activities\", [])\n",
        "        if not items:\n",
        "            break\n",
        "        activities.extend(items)\n",
        "    return activities\n",
        "\n",
        "def chembl_fetch_smiles_for_molecules(mol_ids: List[str], sleep=0.05):\n",
        "    smiles_map = {}\n",
        "    for mid in tqdm(mol_ids, desc=\"Fetching SMILES from ChEMBL\"):\n",
        "        url = f\"{CHEMBL_API}/molecule/{mid}.json\"\n",
        "        r = requests.get(url, timeout=30)\n",
        "        if r.status_code != 200:\n",
        "            continue\n",
        "        js = r.json()\n",
        "        smi = None\n",
        "        try:\n",
        "            smi = js.get(\"molecule_structures\", {}).get(\"canonical_smiles\", None)\n",
        "        except Exception:\n",
        "            smi = None\n",
        "        smiles_map[mid] = smi\n",
        "        time.sleep(sleep)\n",
        "    return smiles_map\n",
        "\n",
        "def load_chembl_data(cache_path):\n",
        "    if os.path.exists(cache_path):\n",
        "        print(f\"Loading cached {TARGET} data from {cache_path}\")\n",
        "        return pd.read_csv(cache_path)\n",
        "    print(f\"Querying ChEMBL for {TARGET} activities...\")\n",
        "    tid = chembl_get_target_id(TARGET)\n",
        "    acts = chembl_fetch_activities_for_target(tid)\n",
        "    rows = []\n",
        "    mol_ids = set()\n",
        "    for a in acts:\n",
        "        pchem = a.get(\"pchembl_value\")\n",
        "        rel = a.get(\"standard_relation\", None)\n",
        "        stype = a.get(\"standard_type\", None)\n",
        "        svalue = a.get(\"standard_value\", None)\n",
        "        sunits = a.get(\"standard_units\", None)\n",
        "        mid = a.get(\"molecule_chembl_id\", None)\n",
        "        if mid is None:\n",
        "            continue\n",
        "        mol_ids.add(mid)\n",
        "        rows.append({\n",
        "            \"molecule_chembl_id\": mid,\n",
        "            \"pchembl_value\": float(pchem) if pchem is not None else np.nan,\n",
        "            \"standard_relation\": rel,\n",
        "            \"standard_type\": stype,\n",
        "            \"standard_value\": float(svalue) if svalue is not None else np.nan,\n",
        "            \"standard_units\": sunits\n",
        "        })\n",
        "    df = pd.DataFrame(rows)\n",
        "    # Fetch SMILES\n",
        "    smiles_map = chembl_fetch_smiles_for_molecules(sorted(mol_ids))\n",
        "    df[\"smiles\"] = df[\"molecule_chembl_id\"].map(smiles_map)\n",
        "    # Filter rows with smiles\n",
        "    df = df[~df[\"smiles\"].isna()].copy()\n",
        "    # Prefer pchembl_value; if missing, try to compute pChEMBL-like value for IC50/Ki in nM\n",
        "    def to_pchembl(row):\n",
        "        if not np.isnan(row[\"pchembl_value\"]):\n",
        "            return row[\"pchembl_value\"]\n",
        "        stype = str(row[\"standard_type\"]).upper() if row[\"standard_type\"] is not None else \"\"\n",
        "        sval = row[\"standard_value\"]\n",
        "        sunits = str(row[\"standard_units\"]).lower() if row[\"standard_units\"] is not None else \"\"\n",
        "        rel = row[\"standard_relation\"]\n",
        "        # Only accept numeric equals or less-than\n",
        "        if rel not in [\"=\", \"<\", \"<=\"]:\n",
        "            return np.nan\n",
        "        if np.isnan(sval):\n",
        "            return np.nan\n",
        "        # Convert to nM when possible\n",
        "        factor = None\n",
        "        if sunits in [\"nm\", \"nanomolar\", \"nм\"]:  # handle common\n",
        "            factor = 1.0\n",
        "        elif sunits in [\"um\", \"µm\", \"micromolar\"]:\n",
        "            factor = 1e3\n",
        "        elif sunits in [\"mm\", \"millimolar\"]:\n",
        "            factor = 1e6\n",
        "        else:\n",
        "            return np.nan\n",
        "        if stype in [\"IC50\", \"KI\", \"EC50\", \"XC50\"]:\n",
        "            # pChEMBL ~ -log10(M)\n",
        "            nM = sval * factor\n",
        "            if nM <= 0:\n",
        "                return np.nan\n",
        "            M = nM * 1e-9\n",
        "            return -math.log10(M)\n",
        "        return np.nan\n",
        "    df[\"pchembl\"] = df.apply(to_pchembl, axis=1)\n",
        "    df = df[~df[\"pchembl\"].isna()].copy()\n",
        "    df.to_csv(cache_path, index=False)\n",
        "    return df\n",
        "\n",
        "# ---------- Task pipelines ----------\n",
        "def prepare_dataframe_generic(df_in: pd.DataFrame, task: str) -> pd.DataFrame:\n",
        "    # Standardize SMILES, deduplicate\n",
        "    df = df_in.copy()\n",
        "    df[\"smiles_std\"] = [standardize_smiles(s) for s in tqdm(df[\"smiles\"].tolist(), desc=f\"Standardizing SMILES for {task}\")]\n",
        "    df = df[~df[\"smiles_std\"].isna()].copy()\n",
        "    df = df.drop_duplicates(subset=[\"smiles_std\"])\n",
        "    df = df.reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "def prepare_bioactivity_classification(pchembl_threshold_active=6.0, min_positives=50):\n",
        "    raw = load_chembl_data(f\"{DATA_DIR}/chembl_cache_{TARGET}.csv\")\n",
        "    df = raw[[\"smiles\", \"pchembl\"]].copy()\n",
        "    df = df.dropna()\n",
        "    # Standardize and aggregate\n",
        "    df[\"smiles_std\"] = [standardize_smiles(s) for s in tqdm(df[\"smiles\"].tolist(), desc=f\"Standardizing SMILES for {TARGET}\")]\n",
        "    df = df[~df[\"smiles_std\"].isna()].copy()\n",
        "    agg = df.groupby(\"smiles_std\")[\"pchembl\"].median().reset_index()\n",
        "    threshold = pchembl_threshold_active\n",
        "    # Adjust threshold if too few positives\n",
        "    n_pos = (agg[\"pchembl\"] >= threshold).sum()\n",
        "    if n_pos < min_positives:\n",
        "        threshold = 5.5\n",
        "        print(f\"{TARGET}: Not enough positives at pChEMBL≥{pchembl_threshold_active}. Relaxing to {threshold}.\")\n",
        "    agg[\"y\"] = (agg[\"pchembl\"] >= threshold).astype(int)\n",
        "    agg = agg.rename(columns={\"smiles_std\": \"smiles\"})\n",
        "    # persist processed dataset\n",
        "    outp = f\"{DATA_DIR}/{TARGET}_processed.csv\"\n",
        "    agg.to_csv(outp, index=False)\n",
        "    print(f\"{TARGET} processed saved to {outp} | N={len(agg)}; Positives={agg['y'].sum()}\")\n",
        "    return agg, threshold\n",
        "\n",
        "def prepare_solubility_regression():\n",
        "    df_raw = load_tdc_solubility()\n",
        "    df = prepare_dataframe_generic(df_raw, \"Solubility\")\n",
        "    df = df.drop(columns=[\"smiles\"]) # CHANGES_MADE_05: Remove original smiles column\n",
        "    df = df.rename(columns={\"smiles_std\": \"smiles\", \"y\": \"logS\"})\n",
        "    outp = f\"{DATA_DIR}/solubility_processed.csv\"\n",
        "    df[[\"smiles\", \"logS\"]].to_csv(outp, index=False)\n",
        "    print(f\"Solubility processed saved to {outp} | N={len(df)}\")\n",
        "    return df[[\"smiles\", \"logS\"]]\n",
        "\n",
        "def prepare_half_life_regression():\n",
        "    df_raw = load_tdc_half_life()\n",
        "    df = prepare_dataframe_generic(df_raw, \"HalfLife\")\n",
        "    df = df.drop(columns=[\"smiles\"]) # CHANGES_MADE_07: Remove original smiles column\n",
        "    # log10-transform (add small epsilon to avoid log of zero)\n",
        "    eps = 1e-6\n",
        "    df[\"log10_half_life\"] = np.log10(df[\"y\"].astype(float) + eps)\n",
        "    df = df.rename(columns={\"smiles_std\": \"smiles\"})\n",
        "    outp = f\"{DATA_DIR}/half_life_processed.csv\"\n",
        "    df[[\"smiles\", \"log10_half_life\"]].to_csv(outp, index=False)\n",
        "    print(f\"Half-Life processed saved to {outp} | N={len(df)}\")\n",
        "    return df[[\"smiles\", \"log10_half_life\"]]\n",
        "\n",
        "def prepare_herg_classification():\n",
        "    df_raw = load_tdc_herg()\n",
        "    df = prepare_dataframe_generic(df_raw, \"hERG\")\n",
        "    df = df.drop(columns=[\"smiles\"]) # CHANGES_MADE_09: Remove original smiles column\n",
        "    df = df.rename(columns={\"smiles_std\": \"smiles\", \"y\": \"label\"})\n",
        "    outp = f\"{DATA_DIR}/herg_processed.csv\"\n",
        "    df[[\"smiles\", \"label\"]].to_csv(outp, index=False)\n",
        "    print(f\"hERG processed saved to {outp} | N={len(df)}; Positives={df['label'].sum()}\")\n",
        "    return df[[\"smiles\", \"label\"]]\n",
        "\n",
        "# ---------- Training and benchmarking ----------\n",
        "def tune_xgb_classifier(X_train, y_train, X_val, y_val, n_iter=20, seed=42, scale_pos_weight=1.0):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    best = None\n",
        "    best_score = -np.inf\n",
        "    for i in range(n_iter):\n",
        "        params = {\n",
        "            \"n_estimators\": int(rng.randint(300, 1200)),\n",
        "            \"max_depth\": int(rng.randint(3, 9)),\n",
        "            \"learning_rate\": 10 ** rng.uniform(-2.0, -0.3),  # 0.01 - 0.5\n",
        "            \"subsample\": rng.uniform(0.6, 1.0),\n",
        "            \"colsample_bytree\": rng.uniform(0.5, 1.0),\n",
        "            \"min_child_weight\": rng.uniform(1.0, 8.0),\n",
        "            \"reg_lambda\": 10 ** rng.uniform(-3, 1),  # 0.001 - 10\n",
        "            \"reg_alpha\": 10 ** rng.uniform(-4, 0),  # 0.0001 - 1\n",
        "            \"gamma\": rng.uniform(0.0, 5.0),\n",
        "            \"scale_pos_weight\": scale_pos_weight,\n",
        "            \"random_state\": seed,\n",
        "            \"eval_metric\": \"aucpr\",\n",
        "            \"n_jobs\": max(1, os.cpu_count() // 2)\n",
        "        }\n",
        "        model = XGBClassifier(**params, tree_method=\"hist\", use_label_encoder=False)\n",
        "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)       # CHANGES_MADE_03: removed early_stopping_rounds=50\n",
        "        proba = model.predict_proba(X_val)[:, 1]\n",
        "        score = average_precision_score(y_val, proba)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best = model\n",
        "    return best\n",
        "\n",
        "def tune_xgb_regressor(X_train, y_train, X_val, y_val, n_iter=20, seed=42):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    best = None\n",
        "    best_score = np.inf  # RMSE\n",
        "    for i in range(n_iter):\n",
        "        params = {\n",
        "            \"n_estimators\": int(rng.randint(300, 1200)),\n",
        "            \"max_depth\": int(rng.randint(3, 9)),\n",
        "            \"learning_rate\": 10 ** rng.uniform(-2.0, -0.3),  # 0.01 - 0.5\n",
        "            \"subsample\": rng.uniform(0.6, 1.0),\n",
        "            \"colsample_bytree\": rng.uniform(0.5, 1.0),\n",
        "            \"min_child_weight\": rng.uniform(1.0, 8.0),\n",
        "            \"reg_lambda\": 10 ** rng.uniform(-3, 1),\n",
        "            \"reg_alpha\": 10 ** rng.uniform(-4, 0),\n",
        "            \"gamma\": rng.uniform(0.0, 5.0),\n",
        "            \"random_state\": seed,\n",
        "            \"objective\": \"reg:squarederror\",\n",
        "            \"n_jobs\": max(1, os.cpu_count() // 2)\n",
        "        }\n",
        "        model = XGBRegressor(**params, tree_method=\"hist\")\n",
        "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)              # CHANGES_MADE_03: removed early_stopping_rounds=50\n",
        "        pred = model.predict(X_val)\n",
        "        rmse = np.sqrt(np.mean((pred - y_val) ** 2))\n",
        "        if rmse < best_score:\n",
        "            best_score = rmse\n",
        "            best = model\n",
        "    return best\n",
        "\n",
        "def find_best_threshold(y_true, y_proba):\n",
        "    # Maximize F1\n",
        "    pr, rc, thr = precision_recall_curve(y_true, y_proba)\n",
        "    # Convert to thresholds on proba grid\n",
        "    # Select threshold that maximizes F1 (on PR curve points, threshold len = len(rc)-1)\n",
        "    best_f1, best_t = 0.0, 0.5\n",
        "    for t in np.linspace(0.05, 0.95, 19):\n",
        "        yp = (y_proba >= t).astype(int)\n",
        "        f1 = f1_score(y_true, yp, zero_division=0)\n",
        "        if f1 > best_f1:\n",
        "            best_f1, best_t = f1, t\n",
        "    return best_t\n",
        "\n",
        "def run_classification_task(name: str, df: pd.DataFrame, label_col: str):\n",
        "    # Build features\n",
        "    smiles = df[\"smiles\"].tolist()\n",
        "    X, df_desc, scaler, fmeta = build_features(smiles)\n",
        "    X[X > 1e5] = 1e5\n",
        "\n",
        "    # for testing\n",
        "    joblib.dump(scaler, f\"models/{name}_desc_scaler.pkl\")\n",
        "    with open(f\"models/{name}_feature_meta.json\", \"w\") as f:\n",
        "        json.dump(fmeta, f, indent=4)\n",
        "\n",
        "    y = df[label_col].values.astype(int)\n",
        "    # Keep descriptor info for permutation importance\n",
        "    n_desc = fmeta[\"n_desc\"]\n",
        "    desc_cols = df_desc.columns.tolist()\n",
        "\n",
        "    # Repeated scaffold splits\n",
        "    metrics_list = []\n",
        "    roc_y_true, roc_y_proba = [], []\n",
        "    cm_images = []\n",
        "    featimp_done = False\n",
        "\n",
        "    for i, seed in enumerate(RANDOM_SEEDS):\n",
        "        print(f\"[{name}] Split {i+1}/{len(RANDOM_SEEDS)} with seed {seed}\")\n",
        "        tr_idx, va_idx, te_idx = scaffold_split_indices(smiles, train_frac=0.72, val_frac=0.08, test_frac=0.20, seed=seed)\n",
        "        X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
        "        X_va, y_va = X[va_idx], y[va_idx]\n",
        "        X_te, y_te = X[te_idx], y[te_idx]\n",
        "\n",
        "        # Class imbalance\n",
        "        n_pos = y_tr.sum()\n",
        "        n_neg = len(y_tr) - n_pos\n",
        "        spw = float(n_neg) / max(1.0, float(n_pos))\n",
        "\n",
        "        # Tune and train\n",
        "        model = tune_xgb_classifier(X_tr, y_tr, X_va, y_va, n_iter=20, seed=seed, scale_pos_weight=spw)\n",
        "        with open(f'models/{name}_{seed}.pkl', 'wb') as f: pickle.dump(model, f)\n",
        "\n",
        "        # Validate threshold\n",
        "        val_proba = model.predict_proba(X_va)[:, 1]\n",
        "        thr = find_best_threshold(y_va, val_proba)\n",
        "        # Test predictions\n",
        "        te_proba = model.predict_proba(X_te)[:, 1]\n",
        "        te_pred = (te_proba >= thr).astype(int)\n",
        "\n",
        "        # Metrics\n",
        "        met = classification_metrics(y_te, te_proba, threshold=thr)\n",
        "        met[\"split_seed\"] = seed\n",
        "        metrics_list.append(met)\n",
        "\n",
        "        roc_y_true.append(y_te)\n",
        "        roc_y_proba.append(te_proba)\n",
        "\n",
        "        # Confusion matrix per split\n",
        "        cm_path = f\"{FIG_DIR}/{name}_cm_split{i+1}.png\"\n",
        "        plot_conf_matrix(y_te, te_pred, f\"{name} Confusion Matrix (split {i+1})\", cm_path)\n",
        "        cm_images.append(cm_path)\n",
        "\n",
        "        # Permutation importance (only once on the first split for descriptor block)\n",
        "        if not featimp_done:\n",
        "            # Build a model on descriptors only to compute interpretable importances\n",
        "            model_desc = XGBClassifier(\n",
        "                n_estimators=model.get_params()[\"n_estimators\"],\n",
        "                max_depth=model.get_params()[\"max_depth\"],\n",
        "                learning_rate=model.get_params()[\"learning_rate\"],\n",
        "                subsample=model.get_params()[\"subsample\"],\n",
        "                colsample_bytree=model.get_params()[\"colsample_bytree\"],\n",
        "                min_child_weight=model.get_params()[\"min_child_weight\"],\n",
        "                reg_lambda=model.get_params()[\"reg_lambda\"],\n",
        "                reg_alpha=model.get_params()[\"reg_alpha\"],\n",
        "                gamma=model.get_params()[\"gamma\"],\n",
        "                scale_pos_weight=spw,\n",
        "                random_state=seed,\n",
        "                eval_metric=\"aucpr\",\n",
        "                n_jobs=max(1, os.cpu_count() // 2),\n",
        "                tree_method=\"hist\",\n",
        "                use_label_encoder=False\n",
        "            )\n",
        "            # Train on descriptor-only slice\n",
        "            X_tr_desc = X_tr[:, :n_desc]\n",
        "            X_te_desc = X_te[:, :n_desc]\n",
        "            model_desc.fit(X_tr_desc, y_tr)\n",
        "            plot_feature_importance(\n",
        "                model_desc, desc_cols, X_te_desc, y_te,\n",
        "                title=f\"{name}: Descriptor Permutation Importance (test split 1)\",\n",
        "                out_path=f\"{FIG_DIR}/{name}_featimp_desc_split1.png\",\n",
        "                n_top=20,\n",
        "                random_state=seed\n",
        "            )\n",
        "            featimp_done = True\n",
        "\n",
        "    # Aggregate metrics\n",
        "    metrics_df = pd.DataFrame(metrics_list)\n",
        "    out_csv = f\"{RESULTS_DIR}/{name}_classification_metrics.csv\"\n",
        "    metrics_df.to_csv(out_csv, index=False)\n",
        "    print(f\"[{name}] Metrics saved to {out_csv}\")\n",
        "    # ROC/PR curves\n",
        "    plot_roc_pr_curves(roc_y_true, roc_y_proba, f\"{name}\", f\"{name}\")\n",
        "    print(f\"[{name}] ROC and PR curves saved to {FIG_DIR}/{name}_*.png\")\n",
        "\n",
        "def run_regression_task(name: str, df: pd.DataFrame, target_col: str, xlabel: str):\n",
        "    smiles = df[\"smiles\"].tolist()\n",
        "    X, df_desc, scaler, fmeta = build_features(smiles)\n",
        "    X[X > 1e5] = 1e5\n",
        "\n",
        "    # for testing\n",
        "    joblib.dump(scaler, f\"models/{name}_desc_scaler.pkl\")\n",
        "    with open(f\"models/{name}_feature_meta.json\", \"w\") as f:\n",
        "        json.dump(fmeta, f, indent=4)\n",
        "\n",
        "    y = df[target_col].values.astype(float)\n",
        "    n_desc = fmeta[\"n_desc\"]\n",
        "    desc_cols = df_desc.columns.tolist()\n",
        "\n",
        "    metrics_list = []\n",
        "    ytrue_list, ypred_list = [], []\n",
        "    featimp_done = False\n",
        "\n",
        "    for i, seed in enumerate(RANDOM_SEEDS):\n",
        "        print(f\"[{name}] Split {i+1}/{len(RANDOM_SEEDS)} with seed {seed}\")\n",
        "        tr_idx, va_idx, te_idx = scaffold_split_indices(smiles, train_frac=0.72, val_frac=0.08, test_frac=0.20, seed=seed)\n",
        "        X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
        "        X_va, y_va = X[va_idx], y[va_idx]\n",
        "        X_te, y_te = X[te_idx], y[te_idx]\n",
        "\n",
        "        model = tune_xgb_regressor(X_tr, y_tr, X_va, y_va, n_iter=20, seed=seed)\n",
        "        with open(f'models/{name}_{seed}.pkl', 'wb') as f: pickle.dump(model, f)\n",
        "\n",
        "        y_pred = model.predict(X_te)\n",
        "\n",
        "        met = regression_metrics(y_te, y_pred)\n",
        "        met[\"split_seed\"] = seed\n",
        "        metrics_list.append(met)\n",
        "\n",
        "        ytrue_list.append(y_te)\n",
        "        ypred_list.append(y_pred)\n",
        "\n",
        "        if not featimp_done:\n",
        "            # Permutation importance on descriptors\n",
        "            model_desc = XGBRegressor(\n",
        "                n_estimators=model.get_params()[\"n_estimators\"],\n",
        "                max_depth=model.get_params()[\"max_depth\"],\n",
        "                learning_rate=model.get_params()[\"learning_rate\"],\n",
        "                subsample=model.get_params()[\"subsample\"],\n",
        "                colsample_bytree=model.get_params()[\"colsample_bytree\"],\n",
        "                min_child_weight=model.get_params()[\"min_child_weight\"],\n",
        "                reg_lambda=model.get_params()[\"reg_lambda\"],\n",
        "                reg_alpha=model.get_params()[\"reg_alpha\"],\n",
        "                gamma=model.get_params()[\"gamma\"],\n",
        "                random_state=seed,\n",
        "                objective=\"reg:squarederror\",\n",
        "                n_jobs=max(1, os.cpu_count() // 2),\n",
        "                tree_method=\"hist\"\n",
        "            )\n",
        "            X_tr_desc = X_tr[:, :n_desc]\n",
        "            X_te_desc = X_te[:, :n_desc]\n",
        "            model_desc.fit(X_tr_desc, y_tr)\n",
        "            plot_feature_importance(\n",
        "                model_desc, desc_cols, X_te_desc, y_te,\n",
        "                title=f\"{name}: Descriptor Permutation Importance (test split 1)\",\n",
        "                out_path=f\"{FIG_DIR}/{name}_featimp_desc_split1.png\",\n",
        "                n_top=20,\n",
        "                random_state=seed\n",
        "            )\n",
        "            featimp_done = True\n",
        "\n",
        "    # Aggregate and save\n",
        "    metrics_df = pd.DataFrame(metrics_list)\n",
        "    out_csv = f\"{RESULTS_DIR}/{name}_regression_metrics.csv\"\n",
        "    metrics_df.to_csv(out_csv, index=False)\n",
        "    print(f\"[{name}] Metrics saved to {out_csv}\")\n",
        "\n",
        "    # Plots\n",
        "    plot_parity(ytrue_list, ypred_list, title=f\"{name}: Parity Plot\", out_path=f\"{FIG_DIR}/{name}_parity.png\", xlabel=xlabel, ylabel=\"Predicted\")\n",
        "    plot_residuals(ytrue_list, ypred_list, title=f\"{name}: Residuals\", out_path=f\"{FIG_DIR}/{name}_residuals.png\")\n",
        "    print(f\"[{name}] Parity and residual plots saved to {FIG_DIR}/{name}_*.png\")\n",
        "\n",
        "def build_features_for_test(smiles: List[str], fmeta, scaler):\n",
        "    df_desc_new, desc_cols_new = compute_descriptors(smiles)\n",
        "    df_desc_new = df_desc_new.reindex(columns=fmeta[\"desc_cols\"], fill_value=0.0)\n",
        "    X_desc_new = scaler.transform(df_desc_new.values.astype(float))\n",
        "    X_fp_new = morgan_fp(smiles, radius=fmeta[\"fp_radius\"], nBits=fmeta[\"fp_nBits\"]).astype(np.float32)\n",
        "    X_new = np.hstack([X_desc_new, X_fp_new])\n",
        "    return X_new, df_desc_new\n",
        "\n",
        "def test_inference(smiles, task_name):\n",
        "    # load scaler + metadata\n",
        "    scaler = joblib.load(f\"models/{task_name}_desc_scaler.pkl\")\n",
        "    with open(f\"models/{task_name}_feature_meta.json\", \"r\") as f:\n",
        "        fmeta = json.load(f)\n",
        "\n",
        "    X_test, df_desc_test = build_features_for_test(smiles, fmeta, scaler)\n",
        "\n",
        "    print()\n",
        "    for s in RANDOM_SEEDS:\n",
        "        with open(f'models/{task_name}_{s}.pkl', 'rb') as f:\n",
        "            loaded_model = pickle.load(f)\n",
        "            print(loaded_model.predict(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKXBwv5Gup7O"
      },
      "source": [
        "# ---------- Main orchestration ----------"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "threshold_dict = {\n",
        "    \"SGLT2\": 7.8,\n",
        "    \"CGAS\": 6.0,\n",
        "    \"SEH\": 7.8,\n",
        "    \"HDAC\": 6.5,\n",
        "    \"DYRK1A\": 7.2\n",
        "}"
      ],
      "metadata": {
        "id": "BNH0tOPVR-_G"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVypJn2nHnYc",
        "outputId": "f41b071f-2359-4ced-f490-5d18f8a304aa"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://www.ebi.ac.uk/chembl/explore/target/CHEMBL3884\n",
            "Loading cached SGLT2 data from data/chembl_cache_SGLT2.csv\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Standardizing SMILES for SGLT2: 100%|██████████| 1762/1762 [00:04<00:00, 370.27it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SGLT2 processed saved to data/SGLT2_processed.csv | N=1437; Positives=638\n",
            "y\n",
            "0    799\n",
            "1    638\n",
            "Name: count, dtype: int64 7.8\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing RDKit 2D descriptors: 100%|██████████| 1437/1437 [00:29<00:00, 48.74it/s]\n",
            "Computing MorganFP r2 n2048: 100%|██████████| 1437/1437 [00:00<00:00, 1955.45it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SGLT2_inhibition] Split 1/5 with seed 13\n",
            "[SGLT2_inhibition] Split 2/5 with seed 17\n",
            "[SGLT2_inhibition] Split 3/5 with seed 23\n",
            "[SGLT2_inhibition] Split 4/5 with seed 29\n",
            "[SGLT2_inhibition] Split 5/5 with seed 31\n",
            "[SGLT2_inhibition] Metrics saved to results/SGLT2_inhibition_classification_metrics.csv\n",
            "[SGLT2_inhibition] ROC and PR curves saved to figures/SGLT2_inhibition_*.png\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing RDKit 2D descriptors: 100%|██████████| 3/3 [00:00<00:00, 149.77it/s]\n",
            "Computing MorganFP r2 n2048: 100%|██████████| 3/3 [00:00<00:00, 2782.60it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[0 0 0]\n",
            "[0 0 0]\n",
            "[0 0 0]\n",
            "[0 0 0]\n",
            "[0 0 0]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://www.ebi.ac.uk/chembl/explore/target/CHEMBL4105728\n",
            "Loading cached CGAS data from data/chembl_cache_CGAS.csv\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Standardizing SMILES for CGAS: 100%|██████████| 361/361 [00:00<00:00, 420.05it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CGAS processed saved to data/CGAS_processed.csv | N=246; Positives=100\n",
            "y\n",
            "0    146\n",
            "1    100\n",
            "Name: count, dtype: int64 6.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing RDKit 2D descriptors: 100%|██████████| 246/246 [00:03<00:00, 66.74it/s]\n",
            "Computing MorganFP r2 n2048: 100%|██████████| 246/246 [00:00<00:00, 2054.82it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CGAS_inhibition] Split 1/5 with seed 13\n",
            "[CGAS_inhibition] Split 2/5 with seed 17\n",
            "[CGAS_inhibition] Split 3/5 with seed 23\n",
            "[CGAS_inhibition] Split 4/5 with seed 29\n",
            "[CGAS_inhibition] Split 5/5 with seed 31\n",
            "[CGAS_inhibition] Metrics saved to results/CGAS_inhibition_classification_metrics.csv\n",
            "[CGAS_inhibition] ROC and PR curves saved to figures/CGAS_inhibition_*.png\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing RDKit 2D descriptors: 100%|██████████| 3/3 [00:00<00:00, 156.58it/s]\n",
            "Computing MorganFP r2 n2048: 100%|██████████| 3/3 [00:00<00:00, 2733.04it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[0 0 0]\n",
            "[0 0 0]\n",
            "[1 1 1]\n",
            "[1 1 1]\n",
            "[0 0 0]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://www.ebi.ac.uk/chembl/explore/target/CHEMBL2409\n",
            "Loading cached SEH data from data/chembl_cache_SEH.csv\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Standardizing SMILES for SEH: 100%|██████████| 2956/2956 [00:08<00:00, 354.63it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SEH processed saved to data/SEH_processed.csv | N=2416; Positives=1032\n",
            "y\n",
            "0    1384\n",
            "1    1032\n",
            "Name: count, dtype: int64 7.8\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing RDKit 2D descriptors: 100%|██████████| 2416/2416 [00:43<00:00, 55.21it/s]\n",
            "Computing MorganFP r2 n2048: 100%|██████████| 2416/2416 [00:01<00:00, 2201.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SEH_inhibition] Split 1/5 with seed 13\n",
            "[SEH_inhibition] Split 2/5 with seed 17\n",
            "[SEH_inhibition] Split 3/5 with seed 23\n",
            "[SEH_inhibition] Split 4/5 with seed 29\n",
            "[SEH_inhibition] Split 5/5 with seed 31\n",
            "[SEH_inhibition] Metrics saved to results/SEH_inhibition_classification_metrics.csv\n",
            "[SEH_inhibition] ROC and PR curves saved to figures/SEH_inhibition_*.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing RDKit 2D descriptors: 100%|██████████| 3/3 [00:00<00:00, 112.66it/s]\n",
            "Computing MorganFP r2 n2048: 100%|██████████| 3/3 [00:00<00:00, 2616.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[0 0 0]\n",
            "[0 0 0]\n",
            "[0 0 0]\n",
            "[0 0 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0]\n",
            "https://www.ebi.ac.uk/chembl/explore/target/CHEMBL6136\n",
            "Loading cached HDAC data from data/chembl_cache_HDAC.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Standardizing SMILES for HDAC: 100%|██████████| 2222/2222 [00:06<00:00, 368.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HDAC processed saved to data/HDAC_processed.csv | N=1656; Positives=696\n",
            "y\n",
            "0    960\n",
            "1    696\n",
            "Name: count, dtype: int64 6.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing RDKit 2D descriptors: 100%|██████████| 1656/1656 [00:55<00:00, 29.97it/s]\n",
            "Computing MorganFP r2 n2048: 100%|██████████| 1656/1656 [00:00<00:00, 1977.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HDAC_inhibition] Split 1/5 with seed 13\n",
            "[HDAC_inhibition] Split 2/5 with seed 17\n",
            "[HDAC_inhibition] Split 3/5 with seed 23\n",
            "[HDAC_inhibition] Split 4/5 with seed 29\n",
            "[HDAC_inhibition] Split 5/5 with seed 31\n",
            "[HDAC_inhibition] Metrics saved to results/HDAC_inhibition_classification_metrics.csv\n",
            "[HDAC_inhibition] ROC and PR curves saved to figures/HDAC_inhibition_*.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing RDKit 2D descriptors: 100%|██████████| 3/3 [00:00<00:00, 187.54it/s]\n",
            "Computing MorganFP r2 n2048: 100%|██████████| 3/3 [00:00<00:00, 2808.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[0 0 0]\n",
            "[0 0 0]\n",
            "[0 0 0]\n",
            "[0 0 0]\n",
            "[0 0 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.ebi.ac.uk/chembl/explore/target/CHEMBL2292\n",
            "Loading cached DYRK1A data from data/chembl_cache_DYRK1A.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Standardizing SMILES for DYRK1A: 100%|██████████| 3256/3256 [00:10<00:00, 304.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DYRK1A processed saved to data/DYRK1A_processed.csv | N=2764; Positives=1211\n",
            "y\n",
            "0    1553\n",
            "1    1211\n",
            "Name: count, dtype: int64 7.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing RDKit 2D descriptors: 100%|██████████| 2764/2764 [00:47<00:00, 58.64it/s]\n",
            "Computing MorganFP r2 n2048: 100%|██████████| 2764/2764 [00:01<00:00, 2195.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DYRK1A_inhibition] Split 1/5 with seed 13\n",
            "[DYRK1A_inhibition] Split 2/5 with seed 17\n",
            "[DYRK1A_inhibition] Split 3/5 with seed 23\n",
            "[DYRK1A_inhibition] Split 4/5 with seed 29\n",
            "[DYRK1A_inhibition] Split 5/5 with seed 31\n",
            "[DYRK1A_inhibition] Metrics saved to results/DYRK1A_inhibition_classification_metrics.csv\n",
            "[DYRK1A_inhibition] ROC and PR curves saved to figures/DYRK1A_inhibition_*.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing RDKit 2D descriptors: 100%|██████████| 3/3 [00:00<00:00, 146.56it/s]\n",
            "Computing MorganFP r2 n2048: 100%|██████████| 3/3 [00:00<00:00, 2835.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[0 0 0]\n",
            "[0 0 0]\n",
            "[0 0 0]\n",
            "[0 0 0]\n",
            "[0 0 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# RANDOM_SEEDS = [13]\n",
        "RANDOM_SEEDS = [13, 17, 23, 29, 31]  # 5 repeats\n",
        "\n",
        "for t in [\"SGLT2\", \"CGAS\", \"SEH\", \"HDAC\", \"DYRK1A\"]:\n",
        "    TARGET = t\n",
        "    print(f\"https://www.ebi.ac.uk/chembl/explore/target/{chembl_get_target_id(TARGET)}\")\n",
        "\n",
        "    training_name = f\"{TARGET}_inhibition\"\n",
        "    bioactivity_df, bioactivity_thr = prepare_bioactivity_classification(pchembl_threshold_active=threshold_dict[TARGET])\n",
        "    print(bioactivity_df[\"y\"].value_counts(), bioactivity_thr)\n",
        "\n",
        "    run_classification_task(training_name, bioactivity_df[[\"smiles\",\"y\"]].rename(columns={\"y\": \"label\"}), label_col=\"label\")\n",
        "\n",
        "    test_smiles = ['CCO', 'CCN', 'CC(=O)O']\n",
        "    test_inference(test_smiles, training_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1ev4ysGx9BE",
        "outputId": "fe5fc2ac-3063-44dc-9a1d-3559e49bdf3b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found local copy...\n",
            "Loading...\n",
            "Done!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading TDC Solubility_AqSolDB...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Standardizing SMILES for Solubility: 100%|██████████| 9982/9982 [00:24<00:00, 408.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Solubility processed saved to data/solubility_processed.csv | N=9458\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing RDKit 2D descriptors: 100%|██████████| 9458/9458 [02:22<00:00, 66.52it/s]\n",
            "Computing MorganFP r2 n2048: 100%|██████████| 9458/9458 [00:04<00:00, 2318.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Solubility_AqSolDB] Split 1/5 with seed 13\n",
            "[Solubility_AqSolDB] Split 2/5 with seed 17\n",
            "[Solubility_AqSolDB] Split 3/5 with seed 23\n",
            "[Solubility_AqSolDB] Split 4/5 with seed 29\n",
            "[Solubility_AqSolDB] Split 5/5 with seed 31\n",
            "[Solubility_AqSolDB] Metrics saved to results/Solubility_AqSolDB_regression_metrics.csv\n",
            "[Solubility_AqSolDB] Parity and residual plots saved to figures/Solubility_AqSolDB_*.png\n"
          ]
        }
      ],
      "source": [
        "# B) Water solubility (regression)\n",
        "sol_df = prepare_solubility_regression()\n",
        "run_regression_task(\"Solubility_AqSolDB\", sol_df, target_col=\"logS\", xlabel=\"logS (mol/L)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_y7I5iPa6ZP",
        "outputId": "64faf0e8-3035-46a9-efbf-cc3043fef34c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing RDKit 2D descriptors: 100%|██████████| 3/3 [00:00<00:00, 133.35it/s]\n",
            "Computing MorganFP r2 n2048: 100%|██████████| 3/3 [00:00<00:00, 2574.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[0.9059403  1.15754    0.49541146]\n",
            "[0.9341944  1.4734123  0.24018402]\n",
            "[0.74191403 0.84268844 0.08887617]\n",
            "[0.7371227  0.82696044 0.42432117]\n",
            "[0.82666254 1.2370104  0.30237755]\n"
          ]
        }
      ],
      "source": [
        "test_inference(smiles, \"Solubility_AqSolDB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ib97aeN2__6Y",
        "outputId": "1710a614-2a4c-4549-a6e8-966b445d4e2f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found local copy...\n",
            "Loading...\n",
            "Done!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading TDC Half_Life_Obach...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Standardizing SMILES for HalfLife: 100%|██████████| 667/667 [00:01<00:00, 387.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Half-Life processed saved to data/half_life_processed.csv | N=665\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing RDKit 2D descriptors: 100%|██████████| 665/665 [00:16<00:00, 41.15it/s]\n",
            "Computing MorganFP r2 n2048: 100%|██████████| 665/665 [00:00<00:00, 1291.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[HalfLife_Human] Split 1/5 with seed 13\n",
            "[HalfLife_Human] Split 2/5 with seed 17\n",
            "[HalfLife_Human] Split 3/5 with seed 23\n",
            "[HalfLife_Human] Split 4/5 with seed 29\n",
            "[HalfLife_Human] Split 5/5 with seed 31\n",
            "[HalfLife_Human] Metrics saved to results/HalfLife_Human_regression_metrics.csv\n",
            "[HalfLife_Human] Parity and residual plots saved to figures/HalfLife_Human_*.png\n"
          ]
        }
      ],
      "source": [
        "# C) Human half-life (regression; log10 transformed)\n",
        "hl_df = prepare_half_life_regression()\n",
        "run_regression_task(\"HalfLife_Human\", hl_df, target_col=\"log10_half_life\", xlabel=\"log10(t1/2)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-POe34yr4v4S",
        "outputId": "4a440fb8-69ff-48f1-b1c4-291a15cf2e58"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing RDKit 2D descriptors: 100%|██████████| 3/3 [00:00<00:00, 25.91it/s]\n",
            "Computing MorganFP r2 n2048: 100%|██████████| 3/3 [00:00<00:00, 500.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[0.507356   0.78613573 0.37344912]\n",
            "[0.44893745 0.50217855 0.2366548 ]\n",
            "[0.5558622  0.644727   0.40902734]\n",
            "[0.5126523  0.5487356  0.44155174]\n",
            "[0.726859   0.82352555 0.6436248 ]\n"
          ]
        }
      ],
      "source": [
        "test_inference(smiles, \"HalfLife_Human\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23ejvmkrquB-",
        "outputId": "3c75c192-bf1a-424e-dba6-a5a38078d70e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading TDC hERG...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 885k/885k [00:00<00:00, 9.33MiB/s]\n",
            "Loading...\n",
            "Done!\n",
            "Standardizing SMILES for hERG: 100%|██████████| 13445/13445 [00:43<00:00, 309.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hERG processed saved to data/herg_processed.csv | N=13152; Positives=6574\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing RDKit 2D descriptors: 100%|██████████| 13152/13152 [04:36<00:00, 47.50it/s]\n",
            "Computing MorganFP r2 n2048: 100%|██████████| 13152/13152 [00:07<00:00, 1863.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[hERG_inhibition] Split 1/5 with seed 13\n",
            "[hERG_inhibition] Split 2/5 with seed 17\n",
            "[hERG_inhibition] Split 3/5 with seed 23\n",
            "[hERG_inhibition] Split 4/5 with seed 29\n",
            "[hERG_inhibition] Split 5/5 with seed 31\n",
            "[hERG_inhibition] Metrics saved to results/hERG_inhibition_classification_metrics.csv\n",
            "[hERG_inhibition] ROC and PR curves saved to figures/hERG_inhibition_*.png\n",
            "\n",
            "All tasks completed. Check the 'results/' CSVs and 'figures/' images.\n"
          ]
        }
      ],
      "source": [
        "# D) hERG inhibition (binary)\n",
        "herg_df = prepare_herg_classification()\n",
        "run_classification_task(\"hERG_inhibition\", herg_df, label_col=\"label\")\n",
        "\n",
        "print(\"\\nAll tasks completed. Check the 'results/' CSVs and 'figures/' images.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSDv1ZZ0a3qk",
        "outputId": "76a28417-1878-4094-b2a8-e8b7ea074b8b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing RDKit 2D descriptors: 100%|██████████| 3/3 [00:00<00:00, 100.77it/s]\n",
            "Computing MorganFP r2 n2048: 100%|██████████| 3/3 [00:00<00:00, 1413.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[0 0 0]\n",
            "[0 0 0]\n",
            "[0 0 0]\n",
            "[0 0 0]\n",
            "[0 0 0]\n"
          ]
        }
      ],
      "source": [
        "test_inference(smiles, \"hERG_inhibition\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}